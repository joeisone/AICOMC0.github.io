# -*- coding: utf-8 -*-
"""Fine Tune with lora.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18WpwkoBk1HCNolktePry2kbaR1gF9qFI
"""

!pip install transformers datasets torch trl peft bitsandbytes

# Load required libraries
from transformers import AutoTokenizer,AutoModelForCausalLM
import torch

# Load model and tokenizer
tokenizer=AutoTokenizer.from_pretrained('deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B')
model=AutoModelForCausalLM.from_pretrained('deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B')

def generate_text(prompt,max_length=500,temperature=0.1):
  """
  Generate text using the Deepseek model.

  Args:
      prompt(str): Input text to generate from
      max_length(int): Maximum length of generated text
      temperature (float): Controls randomness in generation (0.0-1.0)

  Returns:
    str: Generated text

  """
  # Encode the input text
  input_ids=tokenizer(prompt,return_tensors='pt')

  # Generate text
  with torch.no_grad():
    outputs=model.generate(
        input_ids.input_ids,
        max_length=max_length,
        temperature=temperature,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
        num_return_sequences=1
    )
  # Decode and return the generated text
  generated_text=tokenizer.decode(outputs[0],skip_special_tokens=True)
  return generated_text

# Example usage
if __name__=='__main__':
  # Example prompts to test the model

  prompts=[
      "In yoga philosophy, what is the significance of the concept of ahimsa (non-violence)?"
      'Tell me about buddism in India?'
  ]

  print('Generarting text from different prompts:\n')
  for prompt in prompts:
    print(f'Prompt:{prompt}')
    generated=generate_text(prompt)
    print(f'Generated text:{generated}\n')

# Fine tuning

!pip install datasets
!pip install --upgrade trl
from datasets import load_dataset
from transformers import(
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
)

from trl import SFTTrainer
import torch
from peft import LoraConfig,get_peft_model

# Step 1: Load the dataset
dataset=load_dataset('Abhaykoul/Ancient-Indian-Wisdom')

# Step 2: Format the dataset into instruction-response pairs

def format_dataset(examples):
  """Format the dataset into instruction-response pairs."""
  texts=[]
  for instruction,response in zip(examples['instruction'],examples['output']):
      # Combine instruction and response into a single text
      formatted_text=f'### Instruction:\n{instruction}\n\n### Response:\n{response}'
      texts.append(formatted_text)
  return {'text':texts}

# Apply the formatting function to the dataset
dataset=dataset.map(format_dataset,batched=True,remove_columns=dataset['train'].column_names)

# Step 3: load model and tokenizer
model_name='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'
tokenizer=AutoTokenizer.from_pretrained(model_name)
model=AutoModelForCausalLM.from_pretrained(model_name,torch_dtype=torch.float16,device_map='auto')

# Step 4: Configure LoRA
peft_config=LoraConfig(
    r=16, # Rank of low-rank matrices
    lora_alpha=32, # Scaling factor
    lora_dropout=0.1, # Dropout for LoRA layers
    bias='none', # No bias for LoRA
    task_type='CAUSAL_LM', # Task Type
    target_modules=['q_proj','k_proj','v_proj','o_proj'] # Target modules for LoRA
)

model=get_peft_model(model,peft_config)

# Step 5: Define training arguments

training_args=TrainingArguments(
   output_dir='./results',
   num_train_epochs=15,
   per_device_train_batch_size=4,
   per_device_eval_batch_size=4,
   gradient_accumulation_steps=4,
   gradient_checkpointing=False,
   optim='adamw_torch', # optimiser
   learning_rate=1e-4,
   warmup_ratio=0.1,
   fp16=True,
   logging_steps=10,
   eval_strategy='steps',
   eval_steps=100,
   save_strategy='steps',
   save_steps=100,
   logging_dir='./logs',
   eval_accumulation_steps=1,
   load_best_model_at_end=True,
   metric_for_best_model='eval_loss',
   greater_is_better=False,
   remove_unused_columns=True,
   report_to='none'
   )

# Step 6: Initialize trainer

trainer=SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset['train'],
    eval_dataset=dataset['train'].select(range(100)),
    peft_config=peft_config,
    )

# Step 7: Train the model
trainer.train()

# Save model

model.save_pretrained('fine_tuned_model')
tokenizer.save_pretrained('fine_tuned_model')

model_path='fine_tuned_model'

# running the fine tuned model

from transformers import AutoModelForCausalLM,AutoTokenizer
import torch

# Define the path where the fine-tuned model is saved

model_path='fine_tuned_model'

# Define the fine tuned model and tokenizer

model=AutoModelForCausalLM.from_pretrained(model_path)

tokenizer=AutoTokenizer.from_pretrained(model_path)

def generate_text(prompt,max_length=3000):
  inputs=tokenizer(prompt,return_tensors='pt')

  with torch.no_grad():
    outputs=model.generate(**inputs,max_length=max_length,temperature=0.5,top_k=50,top_p=0.9)

  return tokenizer.decode(outputs[0],skip_special_tokens=True)



# Test
prompt='In yoga philosophy,what is the significance of the concept of ahmisa(non-violence)?'
output=generate_text(prompt)
print(output)

prompt2='What was the worst economic year of Ghana?'
output2=generate_text(prompt2)
print(output2)